27 项目实战：福布斯系列
	27.1 数据分析思路
	福布斯每年都会发布福布斯全球上市企业 2000 强排行榜（Forbes Global 2000），这个排行榜每年发布的时候，国内外总有新闻会热闹的讨论一番，但很少⻅到比较全面的分析。
	因此才有了这样一个想法，搜集近些年每年发布的排行榜，做一个进一步的分析。
	在准备做这个小小的项目前，先理了一下整个思路，大概可以分为下面这几个步骤：

1. 数据采集

2. 原始数据完整性检查

3. 数据清洗、整理

4. 从不同⻆度对数据进行分析

5. 数据可视化

6. 总结
  整个分析过程会涉及多篇文章，主要使用 Python 来进行分析。
  • 数据采集: 主要涉及的 python 库包括 requests， BeautifulSoup， csv，以及一些其他常用工具。
  • 数据完整性检查: 包括不同数据来源的对比，以及其他一些常识性的知识。需要对比数据量的多少是否完整，
  以及有些数据是否缺失。
  当然，在拿到数据的初期，其实只能做一个初步的判断，有些内容是在整个分析过程中发现的。
  • 数据清洗与整理: 主要用到 Pandas、 Numpy 以及其他常用库和函数。由于数据比较杂乱，数据清洗与整
  理涉及的内容比较多，可以说是整个福布斯系列的重点之一。
  同时，这个也印证了通常我们所说的数据清洗与整理可能占整个分析的 50~80%。
  • 数据分析与可视化: 经常是伴随在一起的。主要根据不同分析目的进行分析与可视化。用到的工具包括
  Pandas、 Numpy、 Matplotlib、 Seaborn 以及其他一些相关库。
  希望能通过福布斯系列的实战来对数据分析的知识点与工具作一个简单的示例整理与分享。

  

  27.2 数据采集
  27.2.1 数据采集概述
  开始一个数据分析项目，首先需要做的就是 get 到原始数据，获得原始数据的方法有多种途径。比如： 

  ​	1、获取数据集（dataset）文件

  ​	2、使用爬虫采集数据

  ​	3、直接获得 excel、 csv 及其他数据文件

  ​	4、其他途径…
  ​	本次福布斯系列数据分析项目实战，数据采集方面，主要数据来源于使用爬虫进行数据采集，同时也辅助其他数据进行对比。
  ​	本节主要是介绍使用爬虫进行数据采集的思路和步骤。
  ​	本次采集的福布斯全球上市企业 2000 强排行榜数据，涉及年份从 2007 年到 2017 年，跨越 10 多年。
  ​	本次采集的目标网站，是多个网⻚，但多个网⻚的分布结构都有所不同，虽然思路和步骤都差不多，但需要分开来编写，分别采集。

  27.2.2 数据采集步骤
  数据采集大体分为几步：

  ​	1、目标主网⻚内容的 Download

  ​	2、主网⻚上数据的采集

  ​	3、主网⻚上其他分发⻚面网站链接的采集

  ​	4、各分发网⻚数据的 download 与采集

  ​	5、将采集的数据保存
  ​	涉及到的 python 库包括， requests、 BeautifulSoup 以及 csv。下面以采集某年的数据为案例，来描述下数据采集的步骤。 

